"""
Legal QA Chatbot - Streamlit Application

A web-based chatbot for Vietnamese legal Q&A using RAG (Retrieval Augmented Generation)
with Qdrant vector database and Ollama LLM.
"""

import streamlit as st
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from fastembed import SparseTextEmbedding
from fastembed.rerank.cross_encoder import TextCrossEncoder
from qdrant_client import QdrantClient
from qdrant_client.models import models
import uuid
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import logging

# LangChain imports
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document

dense_model_name = "sentence-transformers/all-MiniLM-L6-v2"
sparse_model_name = "Qdrant/bm25"
rerank_model_name = "jinaai/jina-reranker-v2-base-multilingual"
llm_model_name = "llama3.1:8b"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('legal_qa_full.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="Legal QA Chatbot",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        border-left: 4px solid;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left-color: #2196f3;
    }
    .assistant-message {
        background-color: #f3e5f5;
        border-left-color: #9c27b0;
    }
    .source-doc {
        background-color: #fff3e0;
        border-left-color: #ff9800;
        font-size: 0.9rem;
        margin-top: 0.5rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #dee2e6;
    }
</style>
""", unsafe_allow_html=True)

class LegalQASystem:
    """Main class for the Legal QA system."""
    
    def __init__(self):
        self.collection_name = "thue-phi-le-phi_all-MiniLM-L6-v2"
        self.client = None
        self.dense_model = None
        self.sparse_model = None
        self.rerank_model = None
        self.llm = None
        self.initialized = False
    
    def initialize(self):
        """Initialize all models and connections."""
        try:
            with st.spinner("üîÑ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
                # Initialize Qdrant client
                self.client = QdrantClient(
                    host="localhost",
                    port=6333,
                    timeout=600.0
                )
                logger.info("Qdrant client initialized successfully")
                
                # Initialize embedding models
                self.dense_model = SentenceTransformer(dense_model_name)
                logger.info("Dense embedding model (SentenceTransformer) initialized")
                
                self.sparse_model = SparseTextEmbedding(sparse_model_name)
                logger.info("Sparse embedding model (FastEmbed) initialized")
                
                self.rerank_model = TextCrossEncoder(rerank_model_name)
                logger.info("Rerank model (TextCrossEncoder) initialized")
                
                # Initialize LLM
                self.llm = ChatOllama(
                    model=llm_model_name,
                    temperature=0.1,
                    top_p=0.9
                )
                logger.info("LLM (llama3.1:8b) initialized successfully")
                
                self.initialized = True
                logger.info("Full system initialization completed successfully")
                st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
                
        except Exception as e:
            logger.error(f"Failed to initialize full system: {str(e)}", exc_info=True)
            st.error(f"‚ùå L·ªói kh·ªüi t·∫°o h·ªá th·ªëng: {str(e)}")
            self.initialized = False
    
    def retrieve_and_rerank(self, query: str, top_k: int = 20, rerank_top_k: int = 10) -> List[Dict]:
        """Perform hybrid retrieval and reranking."""
        logger.info(f"Starting retrieval and reranking for query: {query[:100]}...")
        logger.info(f"Parameters: top_k={top_k}, rerank_top_k={rerank_top_k}")
        
        if not self.initialized:
            return []
        
        try:
            # Generate query embeddings
            dense_vector_query = self.dense_model.encode(query)
            logger.info(f"Dense vector generated, shape: {dense_vector_query.shape}")
            
            bm25_vector_query = self.sparse_model.embed(query)
            bm25_query_vector = list(bm25_vector_query)[0]
            
            # Perform hybrid search
            logger.info(f"Performing hybrid search on collection: {self.collection_name}")
            search_result = self.client.query_points(
                collection_name=self.collection_name,
                query=models.FusionQuery(
                    fusion=models.Fusion.RRF
                ),
                prefetch=[
                    models.Prefetch(
                        query=dense_vector_query,
                        using="dense",
                        limit=top_k
                    ),
                    models.Prefetch(
                        query=bm25_query_vector.as_object(),
                        using="bm25",
                        limit=top_k
                    ),
                ],
                with_payload=True,
                with_vectors=True,
                limit=top_k
            ).points
            
            logger.info(f"Retrieved {len(search_result)} documents from vector search")
            
            # Extract document texts for reranking
            initial_hits = [hit.payload["raw_context"] for hit in search_result[:rerank_top_k]]
            
            if not initial_hits:
                return []
            
            logger.info(f"Performing reranking on {len(initial_hits)} documents...")
            # Perform reranking
            new_scores = list(self.rerank_model.rerank(query, initial_hits))
            
            # Create ranking with original indices
            ranking = list(enumerate(new_scores))
            ranking.sort(key=lambda x: x[1], reverse=True)
            
            # Prepare results
            results = []
            for rank, (original_idx, rerank_score) in enumerate(ranking, 1):
                original_hit = search_result[original_idx]
                results.append({
                    "rank": rank,
                    "id": original_hit.id,
                    "document": original_hit.payload["raw_context"],
                    "rerank_score": rerank_score,
                    "original_score": original_hit.score if hasattr(original_hit, 'score') else None,
                    "create_at": original_hit.payload.get("create_at", "N/A")
                })
            
            logger.info(f"Reranking completed, returning {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Error in retrieve_and_rerank: {str(e)}", exc_info=True)
            return []
    
    def generate_answer(self, query: str, context_docs: List[str]) -> str:
        """Generate answer using LLM with retrieved context."""
        logger.info(f"Generating answer for query: {query[:100]}...")
        logger.info(f"Number of context documents: {len(context_docs)}")
        
        if not self.initialized or not context_docs:
            return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi v√†o l√∫c n√†y."
        
        try:
            # Create context from retrieved documents
            context = "\n\n".join(context_docs[:3])  # Use top 3 documents
            logger.info(f"Context length: {len(context)} characters")
            
            # Create prompt template
            prompt_template = PromptTemplate(
                input_variables=["context", "question"],
                template="""
B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n ph√°p lu·∫≠t Vi·ªát Nam. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ph√°p l√Ω ƒë∆∞·ª£c cung c·∫•p.

Th√¥ng tin ph√°p l√Ω:
{context}

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
2. S·ª≠ d·ª•ng ng√¥n ng·ªØ d·ªÖ hi·ªÉu, th√¢n thi·ªán
3. N·∫øu c·∫ßn thi·∫øt, h√£y gi·∫£i th√≠ch th√™m v·ªÅ c√°c kh√°i ni·ªám ph√°p l√Ω
4. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥
5. Lu√¥n nh·∫Øc nh·ªü ng∆∞·ªùi d√πng n√™n tham kh·∫£o √Ω ki·∫øn lu·∫≠t s∆∞ cho c√°c v·∫•n ƒë·ªÅ ph·ª©c t·∫°p

Tr·∫£ l·ªùi:
"""
            )
            
            # Create chain
            chain = prompt_template | self.llm | StrOutputParser()
            
            # Generate answer
            answer = chain.invoke({"context": context, "question": query})
            logger.info(f"Generated answer length: {len(answer)} characters")
            logger.info("Answer generation completed successfully")
            return answer
            
        except Exception as e:
            logger.error(f"Error generating answer: {str(e)}", exc_info=True)
            return "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi."

def initialize_session_state():
    """Initialize session state variables."""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "qa_system" not in st.session_state:
        st.session_state.qa_system = LegalQASystem()
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

def display_chat_message(role: str, content: str, sources: List[Dict] = None):
    """Display a chat message with proper styling."""
    if role == "user":
        st.markdown(f"""
        <div class="chat-message user-message">
            <strong>üë§ B·∫°n:</strong><br>
            {content}
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chat-message assistant-message">
            <strong>‚öñÔ∏è Lu·∫≠t s∆∞ AI:</strong><br>
            {content}
        </div>
        """, unsafe_allow_html=True)
        
        # Display sources if available
        if sources:
            with st.expander("üìö Ngu·ªìn tham kh·∫£o", expanded=False):
                for i, source in enumerate(sources[:3], 1):
                    st.markdown(f"""
                    <div class="source-doc">
                        <strong>Ngu·ªìn {i}:</strong><br>
                        {source['document'][:200]}...
                        <br><small>ƒêi·ªÉm s·ªë: {source['rerank_score']:.3f}</small>
                    </div>
                    """, unsafe_allow_html=True)

def main():
    """Main application function."""
    
    # Initialize session state
    initialize_session_state()
    
    # Header
    st.markdown('<h1 class="main-header">‚öñÔ∏è Legal QA Chatbot</h1>', unsafe_allow_html=True)
    st.markdown("### H·ªá th·ªëng t∆∞ v·∫•n ph√°p lu·∫≠t Vi·ªát Nam s·ª≠ d·ª•ng AI")
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C√†i ƒë·∫∑t")
        
        # Initialize system button
        if st.button("üöÄ Kh·ªüi t·∫°o h·ªá th·ªëng", type="primary"):
            st.session_state.qa_system.initialize()
        
        # System status
        if st.session_state.qa_system.initialized:
            st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng")
        else:
            st.warning("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        st.divider()
        
        # Settings
        st.subheader("üîß Tham s·ªë t√¨m ki·∫øm")
        top_k = st.slider("S·ªë l∆∞·ª£ng t√†i li·ªáu t√¨m ki·∫øm", 5, 50, 20)
        rerank_top_k = st.slider("S·ªë l∆∞·ª£ng t√†i li·ªáu rerank", 3, 20, 10)
        
        st.divider()
        
        # Chat history
        st.subheader("üí¨ L·ªãch s·ª≠ chat")
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠"):
            st.session_state.messages = []
            st.session_state.chat_history = []
            st.rerun()
    
    # Main chat interface
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # Chat container
        chat_container = st.container()
        
        # Display chat history
        with chat_container:
            for message in st.session_state.messages:
                display_chat_message(
                    message["role"], 
                    message["content"], 
                    message.get("sources")
                )
        
        # Chat input
        if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi ph√°p lu·∫≠t c·ªßa b·∫°n..."):
            logger.info(f"User entered prompt: {prompt[:100]}...")
            # Add user message to chat history
            st.session_state.messages.append({"role": "user", "content": prompt})
            display_chat_message("user", prompt)
            
            # Generate response
            if st.session_state.qa_system.initialized:
                with st.spinner("üîç ƒêang t√¨m ki·∫øm th√¥ng tin..."):
                    # Retrieve relevant documents
                    retrieved_docs = st.session_state.qa_system.retrieve_and_rerank(
                        prompt, top_k=top_k, rerank_top_k=rerank_top_k
                    )
                    logger.info(f"Retrieved {len(retrieved_docs)} documents")
                
                with st.spinner("ü§ñ ƒêang t·∫°o c√¢u tr·∫£ l·ªùi..."):
                    # Generate answer
                    context_docs = [doc["document"] for doc in retrieved_docs]
                    answer = st.session_state.qa_system.generate_answer(prompt, context_docs)
                    logger.info("Generated answer successfully")
                
                # Add assistant message to chat history
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": answer,
                    "sources": retrieved_docs
                })
                logger.info("Added assistant response to chat history")
                display_chat_message("assistant", answer, retrieved_docs)
            else:
                logger.warning("System not initialized, showing error message")
                error_msg = "‚ö†Ô∏è Vui l√≤ng kh·ªüi t·∫°o h·ªá th·ªëng tr∆∞·ªõc khi s·ª≠ d·ª•ng."
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                display_chat_message("assistant", error_msg)
    
    with col2:
        # Statistics and info
        st.subheader("üìä Th·ªëng k√™")
        
        if st.session_state.qa_system.initialized:
            # Get collection info
            try:
                collection_info = st.session_state.qa_system.client.get_collection(
                    st.session_state.qa_system.collection_name
                )
                
                st.metric("üìÑ T·ªïng t√†i li·ªáu", f"{collection_info.points_count:,}")
                st.metric("üí¨ S·ªë tin nh·∫Øn", len(st.session_state.messages))
                
            except Exception as e:
                st.error(f"L·ªói l·∫•y th√¥ng tin: {str(e)}")
        
        st.divider()
        
        # Quick questions
        st.subheader("‚ùì C√¢u h·ªèi m·∫´u")
        sample_questions = [
            "Thu·∫ø thu nh·∫≠p c√° nh√¢n l√† g√¨?",
            "C√°ch t√≠nh thu·∫ø VAT nh∆∞ th·∫ø n√†o?",
            "Quy ƒë·ªãnh v·ªÅ thu·∫ø thu nh·∫≠p doanh nghi·ªáp?",
            "Th·ªß t·ª•c ƒëƒÉng k√Ω kinh doanh?",
            "Lu·∫≠t lao ƒë·ªông quy ƒë·ªãnh g√¨ v·ªÅ ngh·ªâ ph√©p?"
        ]
        
        for question in sample_questions:
            if st.button(question, key=f"sample_{question}"):
                logger.info(f"User clicked sample question: {question}")
                # Add user message
                st.session_state.messages.append({"role": "user", "content": question})
                logger.info("Added user message to chat history")
                
                # Generate response automatically
                if st.session_state.qa_system.initialized:
                    with st.spinner("üîç ƒêang t√¨m ki·∫øm th√¥ng tin..."):
                        # Retrieve relevant documents
                        retrieved_docs = st.session_state.qa_system.retrieve_and_rerank(
                            question, top_k=top_k, rerank_top_k=rerank_top_k
                        )
                        logger.info(f"Retrieved {len(retrieved_docs)} documents")
                    
                    with st.spinner("ü§ñ ƒêang t·∫°o c√¢u tr·∫£ l·ªùi..."):
                        # Generate answer
                        context_docs = [doc["document"] for doc in retrieved_docs]
                        answer = st.session_state.qa_system.generate_answer(question, context_docs)
                        logger.info("Generated answer successfully")
                    
                    # Add assistant message to chat history
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": answer,
                        "sources": retrieved_docs
                    })
                else:
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                
                st.rerun()
        
        st.divider()
        
        # Footer
        st.markdown("""
        <div style="text-align: center; color: #666; font-size: 0.8rem;">
            <p>‚öñÔ∏è Legal QA Chatbot v1.0</p>
            <p>Powered by RAG + Ollama + Qdrant</p>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
